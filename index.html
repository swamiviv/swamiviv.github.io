
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0//EN">
<html>
<head>
<title>Swami Sankaranarayanan</title>
<style type="text/css">
	body
	{
		width:1400px;
		text-align: center;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight: 300;
		font-size:16px;
		background-color: #FFF;
	}
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	table
	{
		padding: 5px;
	}

	table.pub_table,td.pub_td1,td.pub_td2
	{
		border-collapse: collapse;
		border-bottom: 0px solid #9B9B9B;
		padding-bottom: 10px;
		padding-top: 10px;
		padding-left: 10px;
		width: 1100px;
	}
	td.pub_td1
	{
		width:100px;
	}
	td.pub_td2
	{
	}
	td.year_heading
	{
		color: #3B3B3B;
		font-weight: 700;
		font-size:20px;
	}
	tr {
		background-color: #FFF;
	}

	div#container
	{
		margin-left: auto;
		margin-right: auto;
		width: 1200px;
		text-align: left;
		position: relative;
		background-color: #FFF;
	}
	div#DocInfo
	{
		color: #9B9B9B;
		height: 128px;
	}
	h4,h3,h2,h1
	{
		color: #3B3B3B;
	}
	h2
	{
		font-size:130%;
	}
	p
	{
		color: #000;
		margin-bottom: 20px;
	}
	p.caption
	{
		color: #9B9B9B;
		text-align: left;
		width: 600px;
		font:11px helvetica,sans-serif;
	}
	p.caption2
	{
		color: #9B9B9B;
		text-align: left;
		width: 800px;
		font:11px helvetica,sans-serif;
	}
	#header_img
	{
		position: absolute;
		top: 0px; right: 0px;
	}
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	.section_div {
		background-color: #FFF;
		padding: 10px 10px 10px 10px;
		margin: 10px 10px 10px 10px;
		//border: 1px solid #AAA;
	}
	body {
		background-color: #FFF;
	}
	#personal_info {
		background-color: #FFF;
	}
	p.announcement {
		padding: 10px;
		background-color: #EEE;
	}
	img.teaser_img {
		width: 256px;
		display: block;
    margin-left: auto;
    margin-right: auto;
		margin-top: 5px;
		margin-bottom: 5px;
		border: 0px solid black
	}
	img.teaser_img2 {
		width: 206px;
		display: block;
    margin-left: auto;
    margin-right: auto;
		margin-top: 5px;
		margin-bottom: 5px;
		border: 0px solid black
	}
	img.photo_of_me {
		border-radius: 20px;
	}
	div.teaser_img_div {
		width: 286px;
	}
	table.personnel td {
		padding: 16px;
		vertical-align: top
	}


</style>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-24665197-4', 'auto');
  ga('send', 'pageview');

</script>

</head>


<body>
	<div id="container">

	<div class='section_div'>
	<table id="personal_info">
	<tr>
	<td><img class="profile_pic" src="images/display_pic_bigbend.jpg" width=180px style="border: 1px solid black; float:left; margin-right:15px"/></td>
	<td>
	<div id="DocInfo">
		<h1>Swami Sankaranarayanan</h1>
		s w a m i v i v @ m i t . e d u<br>
		<a href="https://scholar.google.com/citations?user=w3KgvQIAAAAJ&hl=en&oi=ao">Google Scholar</a> / <a href="https://github.com/swamiviv">GitHub</a> / <a href="https://twitter.com/swamiviv1">Twitter</a>
	</div><br>
	</td>
	</tr>
	</table>

<div class='section_div'>
	<h2>About me</h2>
	<p style="color:red";> <strong>I am on the job market -- looking for Faculty positions / early startup positions in trustworthy AI/ML initiatives!</strong> </p>
	<p>
                I am a Postdoctoral Associate at <a href="https://www.mit.edu">MIT</a> working with
                <a href="https://www.web.mit.edu/phillipi">Phillip Isola</a> and
                <a href="https://healthyml.org/">Marzyeh Ghassemi</a>.
        <br><br>
		Previously, I was a Research Scientist at <a href="https:www.butterflynetwork.com">Butterfly Network</a> where I was part
                of the deep learning team led by <a href="https://cs.nyu.edu/~silberman/">Nathan Silberman</a>. I obtained my Ph.D under <a href="https://www.bme.jhu.edu/faculty_staff/rama-chellappa-phd/">Prof. Rama Chellappa</a>,
                for which I was awarded the <a href="https://ece.umd.edu/news/story/ece-names-20172018-distinguished-dissertation-fellows">
                ECE Distinguished Dissertation Award.</a>
		<br>
		<!--<br>I study why we represent the world the way we do, and how we can replicate these abilities in machines.--></p>
	<!--Quick links: <a href="#Papers">Papers</a> / <a href="http://web.mit.edu/phillipi/www/resources.html">Talks</a><br>
	<br>
</div>
<hr>

<!--<p class='announcement'>
	<b>Prospective students:</b> If you are interested in joining my group, there is no need to send an email (and I may not have capacity to reply). Please instead just
	apply through the <a href="https://gradapply.mit.edu/eecs/apply/login/?next=/eecs/">EECS admissions website</a> and indicate
	your interest in my group in your application. This will get to me and makes things easier on my end.
</p>
</div>
<hr>-->

<div class='section_div'>
	&#11088; <h2 style="display: inline;">News</h2> &#11088;<br><br>
	&#x2192; Gave a talk at Google PAIR on our work on semantic uncertainty intervals.<br>
	&#x2192; <strong>Presenting 3 papers at NeurIPS 2022</strong>: 1 main conference and 2 workshops!<br>
	&#x2192; Received the <strong>Scholar Award</strong> to attend NeurIPS 2022!


	<!--&#x2192; We are running a CVPR competition on monitoring deforestation in the Amazon rainforest, using multimodal sensory data. Info and submission website <a href="https://sites.google.com/view/rainforest-challenge">here</a>.<br>-->
	<!--&#x2192; We are running an IJCAI competition on large-scale multiagent learning, on the <a href="https://jsuarez5341.github.io/neural-mmo/build/html/rst/userguide.html">Neural MMO platform</a>. Check it out <a href="https://www.aicrowd.com/challenges/ijcai-2022-the-neural-mmo-challenge">here</a>.-->
	<!--<p>&#9659; New blog post on our work on <a href="https://blog.openai.com/evolved-policy-gradients/">Evolved Policy Gradients</a>.<br>
	&#9659; I am co-organizing a <a href="https://sites.google.com/view/cvpr2018tutorialongans/">tutorial on GANs at CVPR 2018</a>.</p>
	<p>I recently co-organized the <a href="http://vui.eecs.berkeley.edu/">2nd Workshop on Visual Understanding for Interaction</a> at CVPR 2017. Talk slides coming soon!</p>-->
</div>

	<!--<tr><td class="year_heading">2022<hr></td></tr>-->

<div class='section_div' id='Papers'>

	<h2>Papers</h2>
	<hr>
	<table class="pub_table"></table>

	<tr>
	 <!--<td class="pub_td1"><div class="teaser_img_div"><a href="https://jingweim.github.io/totems/"><img class="teaser_img2" src='http://web.mit.edu/phillipi/www/images/totem_teaser.jpg'/></a></div></td>
	 -->
	 <td class="pub_td2"><b>Semantic uncertainty intervals for disentangled latent spaces</b><br>Swami Sankaranarayanan, Anastasios Angelopoulos, Stephen Bates, Yaniv Romoano, Phillip Isola<br>
		 <i>NeurIPS 2022</i>.<br>[<a href="https://arxiv.org/abs/2207.10074">Paper</a>]
		 [<a href="https://swamiviv.github.io/semantic_uncertainty_intervals/">Website</a>]
		 [<a href="https://github.com/swamiviv/generative_semantic_uncertainty">Code</a>]
		 [<a href="https://nips.cc/virtual/2022/poster/53260">Video</a>]
	</td>
	</tr>

	<br>
	<br>

	<tr>
	 <!--<td class="pub_td1"><div class="teaser_img_div"><a href="https://jingweim.github.io/totems/"><img class="teaser_img2" src='http://web.mit.edu/phillipi/www/images/totem_teaser.jpg'/></a></div></td>
	 -->
	 <td class="pub_td2">
		 <b>Real world relevance of counterfactual generations</b><br>Swami Sankaranarayanan, Thomas Hartvigsen, Lauren Oakden-Rayner, Marzyeh Ghassemi, Phillip Isola<br>
		 <i>NeurIPS 2022 Workshop for Trustworthy and Responsible Machine Learning (TSRML).</i>
		 <br>
		 [<a href="https://openreview.net/forum?id=GwvWm56hnN&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DNeurIPS.cc%2F2022%2FWorkshop%2FTSRML%2FAuthors%23your-submissions)">Paper</a>]

	</td>
	</tr>

	<br>
	<br>
	<tr>
	 <!--<td class="pub_td1"><div class="teaser_img_div"><a href="https://jingweim.github.io/totems/"><img class="teaser_img2" src='http://web.mit.edu/phillipi/www/images/totem_teaser.jpg'/></a></div></td>
	 -->
	 <td class="pub_td2">
		 <b>Exploring Visual Prompts for Adapting Large-Scale Models</b><br>Hyojin Bahng, Ali Jahanian*, Swami Sankaranarayanan*, Phillip Isola<br>
		 <i>arXiv 2022</i>.<br>
		 [<a href="https://arxiv.org/abs/2203.17274">Paper</a>]
		 [<a href="https://hjbahng.github.io/visual_prompting/">Website</a>]
		 [<a href="https://github.com/hjbahng/visual_prompting">Code</a>]
	</td>
	</tr>

	<br>
	<br>

	<tr>
	 <!--<td class="pub_td1"><div class="teaser_img_div"><a href="https://jingweim.github.io/totems/"><img class="teaser_img2" src='http://web.mit.edu/phillipi/www/images/totem_teaser.jpg'/></a></div></td>
	 -->
	 <td class="pub_td2">
		 <b>Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors</b><br>Thomas Hartvigsen, <u>Swami Sankaranarayanan</u>, Hamid Palangi, Yoon Kim, Marzyeh Ghassemi<br>
		 <i>Soon to arXiv [also @ NeurIPS 2022 Workshop on Robustness in Sequence Modeling (<p style="color:red;display:inline">Spotlight talk </p>)] </i>.<br>
	</td>
	</tr>

	<br>
	<br>

	<tr>
	 <!--<td class="pub_td1"><div class="teaser_img_div"><a href="https://jingweim.github.io/totems/"><img class="teaser_img2" src='http://web.mit.edu/phillipi/www/images/totem_teaser.jpg'/></a></div></td>
	 -->
	 <td class="pub_td2">
		 <b>Discrepancy Ratio: Evaluating Model Performance When Even Experts Disagree on the Truth</b><br>
		 Igor Lovchinsky, Alon Daks, Israel Malkin, Pouya Samangouei, Ardavan Saeedi, Yang Liu, <u>Swami Sankaranarayanan</u>, <br> Tomer Gafner, Ben Sternlieb, Patrick Maher, Nathan Silberman<br>
		  <i>ICLR 2020</i>.<br>
		 [<a href="https://openreview.net/forum?id=Byg-wJSYDS">Paper</a>]
	</td>
	</tr>

	<br>
	<br>

	<tr>
	 <!--<td class="pub_td1"><div class="teaser_img_div"><a href="https://jingweim.github.io/totems/"><img class="teaser_img2" src='http://web.mit.edu/phillipi/www/images/totem_teaser.jpg'/></a></div></td>
	 -->
	 <td class="pub_td2">
		 <b>Learning From Noisy Labels By Regularized Estimation Of Annotator Confusion</b><br>Ryutaro Tanno, Ardavan Saeedi, <u>Swami Sankaranarayanan</u>, Daniel Alexander, Nathan Silberman<br>
		  <i>CVPR 2019</i>.<br>[<a href="https://arxiv.org/abs/1902.03680">Paper</a>]
		 [<a href="https://rt416.github.io/pdf/trace_codes.pdf">Code</a>]
	</td>
	</tr>

	<br>
	<br>

	<tr>
	 <!--<td class="pub_td1"><div class="teaser_img_div"><a href="https://jingweim.github.io/totems/"><img class="teaser_img2" src='http://web.mit.edu/phillipi/www/images/totem_teaser.jpg'/></a></div></td>
	 -->
	 <td class="pub_td2">
		 <b>MetaReg: Towards Domain Generalization using Meta-Regularization</b><br>Yogesh Balaji, <u>Swami Sankaranarayanan</u>, Rama Chellappa<br>
		  <i>NeurIPS 2018</i>.<br>[<a href="https://papers.nips.cc/paper/7378-metareg-towards-domain-generalization-using-meta-regularization.pdf">Paper</a>]
		 [<a href="https://github.com/yogeshbalaji/Meta-Learning-Domain-Generalization">Code</a>]
	</td>
	</tr>

	<br>
	<br>

	<tr>
	 <!--<td class="pub_td1"><div class="teaser_img_div"><a href="https://jingweim.github.io/totems/"><img class="teaser_img2" src='http://web.mit.edu/phillipi/www/images/totem_teaser.jpg'/></a></div></td>
	 -->
	 <td class="pub_td2">
		 <b>Learning from Synthetic Data (LSD): Addressing Domain Shift for Semantic Segmentation</b><br><u>Swami Sankaranarayanan</u>*, Yogesh Balaji*, Arpit Jain, Sernam Lim, Rama Chellappa<br>
		 <i>CVPR 2018. <p style="color:red;display:inline">[Spotlight talk]</p></i><br>
		 [<a href="https://arxiv.org/abs/1711.06969">Paper</a>]
		 [<a href="https://github.com/swamiviv/LSD-seg">Code</a>]
	</td>
	</tr>

	<br>
	<br>

	<tr>
	 <!--<td class="pub_td1"><div class="teaser_img_div"><a href="https://jingweim.github.io/totems/"><img class="teaser_img2" src='http://web.mit.edu/phillipi/www/images/totem_teaser.jpg'/></a></div></td>
	 -->
	 <td class="pub_td2">
		 <b>Generate To Adapt (GTA): Aligning Domains using Generative Adversarial Networks</b><br><u>Swami Sankaranarayanan</u>*, Yogesh Balaji*, Carlos Castillo, Rama Chellappa<br>
		 <i>CVPR 2018. <p style="color:red;display:inline">[Spotlight talk]</p></i><br>
		 [<a href="https://arxiv.org/abs/1704.01705">Paper</a>]
		 [<a href="https://github.com/yogeshbalaji/Generate_To_Adapt">Code</a>]
	</td>
	</tr>

	<br>
	<br>

	<tr>
	 <!--<td class="pub_td1"><div class="teaser_img_div"><a href="https://jingweim.github.io/totems/"><img class="teaser_img2" src='http://web.mit.edu/phillipi/www/images/totem_teaser.jpg'/></a></div></td>
	 -->
	 <td class="pub_td2">
		 <b>Face recognition accuracy of forensic examiners, superrecognizers, and face recognition algorithms</b>
		 <br><u>Swami Sankaranarayanan</u>*, Yogesh Balaji*, Carlos Castillo, Rama Chellappa<br>
		 <i>CVPR 2018. <p style="color:red;display:inline">[Spotlight talk]</p></i><br>
		 [<a href="https://arxiv.org/abs/1704.01705">Paper</a>]
		 [<a href="https://github.com/yogeshbalaji/Generate_To_Adapt">Code</a>]
	</td>
	</tr>

</div>

<!--

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://chail.github.io/anyres-gan/"><img class="teaser_img2" src='http://web.mit.edu/phillipi/www/images/anyres_teaser.jpg'/></a></div></td>
	 <td class="pub_td2"><b>Any-resolution Training for High-resolution Image Synthesis</b><br>Lucy Chai, Micha&#235;l Gharbi, Eli Shechtman, Phillip Isola, Richard Zhang<br><i>ECCV 2022</i>.<br>[<a href="https://arxiv.org/abs/2204.07156">Paper</a>][<a href="https://chail.github.io/anyres-gan/">Website</a>][<a href="https://github.com/chail/anyres-gan">Code</a>][<a href="http://latent-composition.csail.mit.edu/other_projects/anyres_gan/supplementary_video_v4_1600_noaudio.mp4">Video</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://ssnl.github.io/denoised_mdp/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/denoised_mdps_teaser.jpg'/></a></div></td>
	 <td class="pub_td2"><b>Denoised MDPs: Learning World Models Better Than The World Itself</b><br>Tongzhou Wang, Simon S. Du, Antonio Torralba, Phillip Isola, Amy Zhang, Yuandong Tian<br><i>ICML 2022</i>.<br>[<a href="https://arxiv.org/abs/2206.15477">Paper</a>][<a href="https://ssnl.github.io/denoised_mdp/">Website</a>][<a href="https://github.com/facebookresearch/denoised_mdp">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://hjbahng.github.io/visual_prompting/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/vp_teaser_v3.jpg'/></a></div></td>
	 <td class="pub_td2"><b>Exploring Visual Prompts for Adapting Large-Scale Models</b><br>Hyojin Bahng, Ali Jahanian*, Swami Sankaranarayanan*, Phillip Isola<br><i>arXiv 2022</i>.<br>[<a href="https://arxiv.org/abs/2203.17274">Paper</a>][<a href="https://hjbahng.github.io/visual_prompting/">Website</a>][<a href="https://github.com/hjbahng/visual_prompting">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://carolineec.github.io/informative_drawings/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/informative_drawings_teaser.jpg'/></a></div></td>
	 <td class="pub_td2"><b>Learning to generate line drawings that convey geometry and semantics</b><br>Caroline Chan, Fr&eacute;do Durand, Phillip Isola<br><i>CVPR 2022</i>.<br>[<a href="https://arxiv.org/abs/2203.12691">Paper</a>][<a href="https://carolineec.github.io/informative_drawings/">Website</a>][<a href="https://github.com/carolineec/informative-drawings">Code</a>][<a href="https://huggingface.co/spaces/carolineec/informativedrawings">Demo</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://ssnl.github.io/quasimetric/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/qmets_teaser.jpg'/></a></div></td>
	 <td class="pub_td2"><b>On the Learning and Learnability of Quasimetrics</b><br>Tongzhou Wang, Phillip Isola<br><i>ICLR 2022</i>.<br>[<a href="https://openreview.net/pdf?id=y0VvIg25yk">Paper</a>][<a href="https://ssnl.github.io/quasimetric/">Website</a>][<a href="https://github.com/SsnL/poisson_quasimetric_embedding">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://ali-design.github.io/GenRep/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/genrep_teaser.jpg'/></a></div></td>
	 <td class="pub_td2"><b>Generative Models as a Data Source for Multiview Representation Learning</b><br>Ali Jahanian, Xavier Puig, Yonglong Tian, Phillip Isola<br><i>ICLR 2022</i>.<br>[<a href="https://arxiv.org/abs/2106.05258">Paper</a>][<a href="https://ali-design.github.io/GenRep/">Website</a>][<a href="https://github.com/ali-design/GenRep">Code</a>][<a href="https://news.mit.edu/2022/synthetic-datasets-ai-image-classification-0315">News article</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://yenchenlin.me/nerf-supervision/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/nerf_supervision.gif'/></a></div></td>
	 <td class="pub_td2"><b>NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields</b><br>Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Tsung-Yi Lin, Alberto Rodriguez, Phillip Isola<br><i>ICRA 2022</i>.<br>[<a href="https://arxiv.org/abs/2203.01913">Paper</a>][<a href="https://yenchenlin.me/nerf-supervision/">Website</a>][<a href="https://www.youtube.com/watch?v=_zN-wVwPH1s&ab_channel=Yen-ChenLin">Video</a>][<a href="https://github.com/yenchenlin/nerf-supervision-public">Code</a>][<a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">Colab</a>]
	</td></tr>


  <tr><td class="year_heading">2021<hr></td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://toruowo.github.io/marl-ae-comm/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/ae_comm_teaser2.png'/></a></div></td>
	 <td class="pub_td2"><b>Learning to Ground Multi-Agent Communication with Autoencoders</b><br>Toru Lin, Jacob Huh, Chris Stauffer, Sernam Lim, Phillip Isola<br><i>NeurIPS 2021</i>.<br>[<a href="https://arxiv.org/abs/2110.15349">Paper</a>][<a href="https://toruowo.github.io/marl-ae-comm/">Website</a>][<a href="https://github.com/ToruOwO/marl-ae-comm">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://jsuarez5341.github.io/neural-mmo/build/html/rst/userguide.html"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/neural_mmo_neurips2021_teaser.jpg'/></a></div></td>
	 <td class="pub_td2"><b>The Neural MMO Platform for Massively Multiagent Research</b><br>Joseph Suarez, Yilun Du, Clare Zhu, Igor Mordatch, Phillip Isola<br><i>NeurIPS 2021, Track on Datasets and Benchmarks</i>.<br>[<a href="https://openreview.net/forum?id=J0d-I8yFtP">Pre-print</a>][<a href="https://jsuarez5341.github.io/neural-mmo/build/html/rst/userguide.html">Website</a>][<a href="https://github.com/jsuarez5341/neural-mmo">Code</a>][<a href="https://www.aicrowd.com/challenges/the-neural-mmo-challenge">Competition</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://kennyderek.github.io/adap/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/adap_teaser.png'/></a></div></td>
	 <td class="pub_td2"><b>Adaptable Agent Populations Using a Generative Model of Policies</b><br>Kenneth Derek, Phillip Isola<br><i>NeurIPS 2021</i>.<br>[<a href="https://arxiv.org/abs/2107.07506">Paper</a>][<a href="https://kennyderek.github.io/adap/">Website</a>][<a href="https://github.com/kennyderek/adap">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://mbaradad.github.io/learning_with_noise/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/noise_teaser2.jpg'/></a></div></td>
	 <td class="pub_td2"><b>Learning to See by Looking at Noise</b><br>Manel Baradad*, Jonas Wulff*, Tongzhou Wang, Phillip Isola, Antonio Torralba<br><i>NeurIPS 2021 (spotlight)</i>.<br>[<a href="https://arxiv.org/abs/2106.05963">Paper</a>][<a href="https://mbaradad.github.io/learning_with_noise/">Website</a>][<a href="https://github.com/mbaradad/learning_with_noise">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://explaining-in-style.github.io/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/stylex_teaser.gif'/></a></div></td>
	 <td class="pub_td2"><b>Explaining in Style: Training a GAN to explain a classifier in StyleSpace</b><br>Oran Lang*, Yossi Gandelsman*, Michal Yarom*, Yoav Wald*, Gal Elidan, Avinatan Hassidim, William T. Freeman, Phillip Isola, Amir Globerson, Michal Irani, Inbar Mosseri<br><i>ICCV 2021</i>.<br>[<a href="https://arxiv.org/abs/2104.13369">Paper</a>][<a href="https://explaining-in-style.github.io/">Website</a>][<a href="https://github.com/google/explaining-in-style">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://yilundu.github.io/crl/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/CRL_teaser.jpg'/></a></div></td>
	 <td class="pub_td2"><b>Curious Representation Learning for Embodied Intelligence</b><br>Yilun Du, Chuang Gan, Phillip Isola<br><i>ICCV 2021</i>.<br>[<a href="https://arxiv.org/abs/2105.01060">Paper</a>][<a href="https://yilundu.github.io/crl/">Website</a>][<a href="https://github.com/yilundu/crl">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="http://yenchenlin.me/inerf/"><img class="teaser_img" src='https://yenchenlin.me/assets/inerf.gif'/></a></div></td>
	 <td class="pub_td2"><b>iNeRF: Inverting Neural Radiance Fields for Pose Estimation</b><br>Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto Rodriguez, Phillip Isola, Tsung-Yi Lin<br><i>IROS 2021</i>.<br>[<a href="https://arxiv.org/abs/2012.05877">Paper</a>][<a href="http://yenchenlin.me/inerf/">Website</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://chail.github.io/gan-ensembling/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/gan_ensembling.jpg'/></a></div></td>
	 <td class="pub_td2"><b>Ensembling with Deep Generative Views</b><br>Lucy Chai, Jun-Yan Zhu, Eli Shechtman, Phillip Isola, Richard Zhang<br><i>CVPR 2021</i>.<br>[<a href="https://arxiv.org/abs/2104.14551">Paper</a>][<a href="https://chail.github.io/gan-ensembling/">Website</a>][<a href="https://github.com/chail/gan-ensembling">Code</a>][<a href="https://colab.research.google.com/drive/1-qZBjn07KlWv27kKQGaKOXMBgP-Fb0Ws?usp=sharing">Colab</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://minyoungg.github.io/overparam/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/overparam_teaser.png'/></a></div></td>
	 <td class="pub_td2"><b>The Low-Rank Simplicity Bias in Deep Networks</b><br>Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, Phillip Isola<br><i>arXiv 2021</i>.<br>[<a href="https://arxiv.org/abs/2103.10427">Paper</a>][<a href="https://minyoungg.github.io/overparam/">Website</a>][<a href="https://github.com/minyoungg/overparam">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://chail.github.io/latent-composition/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/latent_composition_initial.png'/></a></div></td>
	 <td class="pub_td2"><b>Using latent space regression to analyze and leverage compositionality in GANs</b><br>Lucy Chai, Jonas Wulff, Phillip Isola<br><i>ICLR 2021</i>.<br>[<a href="https://arxiv.org/abs/2103.10426">Paper</a>][<a href="https://chail.github.io/latent-composition/">Website</a>][<a href="https://github.com/chail/latent-composition">Code</a>][<a href="https://colab.research.google.com/drive/1p-L2dPMaqMyr56TYoYmBJhoyIyBJ7lzH?usp=sharing">Demo (Colab)</a>]
	</td></tr>

  <tr><td class="year_heading">2020<hr></td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="http://noisy-agent.csail.mit.edu/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/noisy_agents_teaser.png'/></a></div></td>
	 <td class="pub_td2"><b>Noisy Agents: Self-supervised Exploration by Predicting Auditory Events</b><br>Chuang Gan, Xiaoyu Chen, Phillip Isola, Antonio Torralba, Joshua B. Tenenbaum<br><i>arXiv 2020</i>.<br>[<a href="https://arxiv.org/abs/2007.13729">Paper</a>][<a href="http://noisy-agent.csail.mit.edu/">Website</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://hobbitlong.github.io/InfoMin/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/infomin_teaser.png'/></a></div></td>
	 <td class="pub_td2"><b>What makes for good views for contrastive learning?</b><br>Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, Phillip Isola<br><i>NeurIPS 2020</i>.<br>[<a href="https://arxiv.org/abs/2005.10243">Paper</a>][<a href="https://hobbitlong.github.io/InfoMin/">Website</a>][<a href="https://github.com/HobbitLong/PyContrast">Code</a>][<a href="https://ai.googleblog.com/2020/08/understanding-view-selection-for.html">Blog</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://arxiv.org/abs/2004.11362"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/SupCon_teaser.jpg'/></a></div></td>
	 <td class="pub_td2"><b>Supervised Contrastive Learning</b><br>Prannay Khosla*, Piotr Teterwak*, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan<br><i>NeurIPS 2020</i>.<br>[<a href="https://arxiv.org/abs/2004.11362">Paper</a>][<a href="https://github.com/HobbitLong/SupContrast">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://chail.github.io/patch-forensics/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/patch_forensics_teaser.jpg'/></a></div></td>
	 <td class="pub_td2"><b>What makes fake images detectable? Understanding properties that generalize</b><br>Lucy Chai, David Bau, Ser-Nam Lim, Phillip Isola<br><i>ECCV 2020</i>.<br>[<a href="https://arxiv.org/abs/2008.10588">Paper</a>][<a href="https://chail.github.io/patch-forensics/">Website</a>][<a href="https://github.com/chail/patch-forensics">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://people.csail.mit.edu/yuewang/projects/rfs/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/good_embedding_teaser.png'/></a></div></td>
	 <td class="pub_td2"><b>Rethinking Few-Shot Image Classification: A Good Embedding Is All You Need?</b><br>Yonglong Tian*, Yue Wang*, Dilip Krishnan, Josh Tenenbaum, Phillip Isola<br><i>ECCV 2020</i>.<br>[<a href="https://arxiv.org/abs/2003.11539">Paper</a>][<a href="https://people.csail.mit.edu/yuewang/projects/rfs/">Website</a>][<a href="https://github.com/WangYueFt/rfs">Code</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="https://hobbitlong.github.io/CMC/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/CMC_teaser.jpg'/></a></div></td>
		<td class="pub_td2"><b>Contrastive Multiview Coding</b><br>Yonglong Tian, Dilip Krishnan, Phillip Isola<br><i>ECCV 2020</i>.<br>[<a href="https://arxiv.org/abs/1906.05849">Paper</a>][<a href="https://hobbitlong.github.io/CMC/">Website</a>][<a href="https://github.com/HobbitLong/CMC/">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://ssnl.github.io/hypersphere/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/align_uniform_teaser.jpg'/></a></div></td>
	 <td class="pub_td2"><b>Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere</b><br>Tongzhou Wang, Phillip Isola<br><i>ICML 2020</i>.<br>[<a href="https://arxiv.org/abs/2005.10242">Paper</a>][<a href="https://ssnl.github.io/hypersphere/">Website</a>][<a href="https://github.com/SsnL/align_uniform">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://github.com/jsuarez5341/neural-mmo"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/neural_mmo_v1.3.jpg'/></a></div></td>
	 <td class="pub_td2"><b>Neural MMO v1.3: A Massively Multiagent Game Environment for Training and Evaluating Neural Networks</b><br>Joseph Suarez, Yilun Du, Igor Mordatch, Phillip Isola<br><i>AAMAS Extended Abstract 2020</i>.<br>[<a href="http://ifaamas.org/Proceedings/aamas2020/pdfs/p2020.pdf">Extended abstract</a>][<a href="https://arxiv.org/abs/2001.12004">arXiv paper</a>][<a href="https://github.com/jsuarez5341/neural-mmo">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://yenchenlin.me/vision2action/"><img class="teaser_img" src='https://yenchenlin.me/assets/vision2action.png'/></a></div></td>
	 <td class="pub_td2"><b>Learning to See before Learning to Act: Visual Pre-training for Manipulation</b><br>Lin Yen-Chen, Andy Zeng, Shuran Song, Phillip Isola, Tsung-Yi Lin<br><i>ICRA 2020</i>.<br>[<a href="https://drive.google.com/file/d/1D0d2plVlvdk0ltGSTK7940TCs3CGTgKy/view">Paper</a>][<a href="https://yenchenlin.me/vision2action/">Website</a>][<a href="https://github.com/yenchenlin/vision2action">Code</a>][<a href="https://www.youtube.com/watch?v=7tFO2V0sYJg&feature=emb_logo">Video</a>][<a href="https://ai.googleblog.com/2020/03/visual-transfer-learning-for-robotic.html">Blog</a>]
	</td></tr>

	<tr>
 	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://hobbitlong.github.io/CRD/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/CRD_teaser.png'/></a></div></td>
 	 <td class="pub_td2"><b>Contrastive Representation Distillation</b><br>Yonglong Tian, Dilip Krishnan, Phillip Isola<br><i>ICLR 2020</i>.<br>[<a href="https://arxiv.org/abs/1910.10699">Paper</a>][<a href="https://hobbitlong.github.io/CRD/">Website</a>][<a href="https://github.com/HobbitLong/RepDistiller">Code</a>]
  </td></tr>

	<tr>
 	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://ali-design.github.io/gan_steerability/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/steerability_teaser.jpg'/></a></div></td>
 	 <td class="pub_td2"><b>On the "steerability" of generative adversarial networks</b><br>Ali Jahanian*, Lucy Chai*, Phillip Isola<br><i>ICLR 2020</i>.<br>[<a href="https://arxiv.org/abs/1907.07171">Paper</a>][<a href="https://ali-design.github.io/gan_steerability/">Website</a>][<a href="https://github.com/ali-design/gan_steerability">Code</a>]
  </td></tr>


	<tr><td class="year_heading"><br>2019<hr></td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="https://arxiv.org/abs/1903.00784"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/neural_mmo_teaser.jpg'/></a></div></td>
		<td class="pub_td2"><b>Neural MMO: A Massively Multiagent Game Environment for Training and Evaluating Intelligent Agents</b><br>Joseph Suarez, Yilun Du, Phillip Isola, Igor Mordatch<br><i>arXiv 2019</i>.<br>[<a href="https://arxiv.org/abs/1903.00784">Paper</a>][<a href="https://openai.com/blog/neural-mmo/">Blog</a>][<a href="https://github.com/openai/neural-mmo">Code</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="https://pathak22.github.io/modular-assemblies/"><img class="teaser_img" src='https://www.cs.cmu.edu/~dpathak/images/assemblies19.gif'/></a></div></td>
		<td class="pub_td2"><b>Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity </b><br>Deepak Pathak*, Chris Lu*, Trevor Darrell, Phillip Isola, Alexei A. Efros<br><i>NeurIPS 2019</i>.<br>Winner of Virtual Creatures Competition at GECCO 2019 (<a href="https://virtualcreatures.github.io">link</a>)<br>[<a href="https://arxiv.org/abs/1902.05546">Paper</a>][<a href="https://pathak22.github.io/modular-assemblies/">Website</a>][<a href="https://github.com/pathak22/modular-assemblies/">Code</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://ganalyze.csail.mit.edu/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/ganalyze_teaser.jpg'/></a></div></td>
		<td class="pub_td2"><b>GANalyze: Toward Visual Definitions of Cognitive Image Properties</b><br>Lore Goetschalckx*, Alex Andonian, Aude Oliva, Phillip Isola<br><i>ICCV 2019</i>.<br>[<a href="https://arxiv.org/abs/1906.10112">Paper</a>][<a href="http://ganalyze.csail.mit.edu/">Website</a>][<a href="https://github.com/LoreGoetschalckx/GANalyze">Code</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://www.wisdom.weizmann.ac.il/~vision/ingan/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/ingan_teaser.gif'/></a></div></td>
		<td class="pub_td2"><b>InGAN: Capturing and Remapping the "DNA" of a Natural Image</b><br>Assaf Shocher, Shai Bagon, Phillip Isola, Michal Irani<br><i>ICCV 2019 (oral)</i>.<br>[<a href="https://arxiv.org/abs/1812.00231">Paper</a>][<a href="http://www.wisdom.weizmann.ac.il/~vision/ingan/">Website</a>][<a href="https://github.com/assafshocher/InGAN">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="http://yenchenlin.me/evf/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/EVF_teaser.jpg'/></a></div></td>
	 <td class="pub_td2"><b>Experience-embedded Visual Foresight</b><br>Lin Yen-Chen, Maria Bauza, Phillip Isola<br><i>CoRL 2019</i>.<br>[<a href="https://arxiv.org/abs/1911.05071">Paper</a>][<a href="http://yenchenlin.me/evf/">Website</a>][<a href="https://github.com/yenchenlin/evf-public">Code</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://web.mit.edu/mcube/omnipush-dataset/index.html"><img class="teaser_img" src='http://yenchenlin.me/assets/omnipush.gif'/></a></div></td>
		<td class="pub_td2"><b>Omnipush: accurate, diverse, real-world dataset of pushing dynamics with RGBD images</b><br>Maria Bauza, Ferran Alet, Yen-Chen Lin, Tomas Lozano-Perez, Leslie P. Kaelbling,
Phillip Isola, Alberto Rodriguez<br><i>IROS 2019</i>.<br>[<a href="https://arxiv.org/abs/1910.00618">Paper</a>][<a href="http://web.mit.edu/mcube/omnipush-dataset/index.html">Website</a>]
	</td></tr>


	<tr><td class="year_heading"><br>2018<hr></td></tr>

	<tr>
				<td class="pub_td1"><div class="teaser_img_div"><a href="https://blog.openai.com/evolved-policy-gradients/"><img class="teaser_img" src='https://storage.googleapis.com/epg-blog-data/fig1_4.gif'/></a></div></td>
		<td class="pub_td2"><b>Evolved Policy Gradients</b><br>Rein Houthooft, Richard Y. Chen, Phillip Isola, Bradly C. Stadie, Filip Wolski, Jonathan Ho, Pieter Abbeel<br><i>NeurIPS 2018</i>.<br>[<a href="https://arxiv.org/abs/1802.04821">Paper</a>][<a href="https://blog.openai.com/evolved-policy-gradients/">Blog</a>][<a href="https://github.com/openai/EPG">Code</a>]
	</td></tr>

	<tr>
        <td class="pub_td1"><div class="teaser_img_div"><a href="https://arxiv.org/abs/1711.03213"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/cycada_teaser.jpg'/></a></div></td>
		<td class="pub_td2"><b>CyCADA: Cycle-Consistent Adversarial Domain Adaptation</b><br>Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A. Efros, Trevor Darrell<br><i>ICML 2018</i>.<br>[<a href="https://arxiv.org/abs/1711.03213">Paper</a>]
	</td></tr>

	<tr>
        <td class="pub_td1"><div class="teaser_img_div"><a href="https://psyarxiv.com/p6kv9/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/memorable_words_teaser.jpg'/></a></div></td>
		<td class="pub_td2"><b>Memorable words are monogamous: The role of synonymy and homonymy in word recognition memory</b><br>Kyle Mahowald*, Phillip Isola*, Evelina Fedorenko, Edward Gibson, Aude Oliva<br><i>PsyArXiv 2018</i>.<br>[<a href="https://psyarxiv.com/p6kv9/">Paper</a>]
	</td></tr>

	<tr>
        <td class="pub_td1"><div class="teaser_img_div"><a href="https://richzhang.github.io/PerceptualSimilarity/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/perceptual_teaser.jpg'/></a></div></td>
		<td class="pub_td2"><b>The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</b><br>Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang<br><i>CVPR 2018</i>.<br>[<a href="https://arxiv.org/abs/1801.03924">Paper</a>][<a href="https://richzhang.github.io/PerceptualSimilarity/">Website</a>][<a href="https://github.com/richzhang/PerceptualSimilarity">Code</a>]
	</td></tr>

	<tr>
        <td class="pub_td1"><div class="teaser_img_div"><a href="https://arxiv.org/abs/1707.08390"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/3DSketch.jpg'/></a></div></td>
		<td class="pub_td2"><b>What You Sketch Is What You Get: 3D Sketching using Multi-View Deep Volumetric Prediction</b><br>Johanna Delanoy, Adrien Bousseau, Mathieu Aubry, Phillip Isola, Alexei A. Efros<br><i>I3D 2018</i>.<br>[<a href="https://arxiv.org/abs/1707.08390">Paper</a>][<a href="https://www.youtube.com/watch?v=DGIYzmlm2pQ">Video</a>]
	</td></tr>

  <tr><td class="year_heading"><br>2017<hr></td></tr>
	<tr>
        <td class="pub_td1"><div class="teaser_img_div"><a href="https://junyanz.github.io/CycleGAN/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/cyclegan_teaser3.jpg'/></a></div></td>
		<td class="pub_td2"><b>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</b><br>Jun-Yan Zhu*, Taesung Park*, Phillip Isola, Alexei A. Efros<br><i>ICCV 2017</i>.<br>[<a href="https://arxiv.org/abs/1703.10593">Paper</a>][<a href="https://junyanz.github.io/CycleGAN/">Website</a>][<a href="https://github.com/junyanz/CycleGAN">Code</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="https://ropemanipulation.github.io/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/rope_teaser.jpg'/></a></div></td>
		<td class="pub_td2"><b>Combining Self-Supervised Learning and Imitation for Vision-Based Rope Manipulation</b><br>Ashvin Nair, Pulkit Agrawal, Dian Chen, Phillip Isola, Pieter Abbeel, Jitendra Malik, Sergey Levine<br><i>ICRA 2017</i>.<br>[<a href="https://arxiv.org/abs/1703.02018">Paper</a>][<a href="https://ropemanipulation.github.io/">Website</a>][<a href="https://www.youtube.com/watch?v=ofNQh5ELrOw">Video</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="https://richzhang.github.io/InteractiveColorization"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/icolor_teaser.jpg'/></a></div></td>
		<td class="pub_td2">Richard Zhang*, Jun-Yan Zhu*, Phillip Isola, Xinyang Geng, Angela S. Lin, Tianhe Yu, Alexei A. Efros<br><b>Real-Time User-Guided Image Colorization with Learned Deep Priors</b><br><i>SIGGRAPH 2017</i>.<br>[<a href="https://arxiv.org/abs/1705.02999">Paper</a>][<a href="https://richzhang.github.io/InteractiveColorization">Website</a>][<a href="https://github.com/junyanz/interactive-deep-colorization">Code</a>][<a href="https://www.youtube.com/watch?v=eL5ilZgM89Q&feature=youtu.be">Video</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://people.csail.mit.edu/jahanian/papers/JahanianIsolaWei-designEvolution-CHI2017-LBW-camera-ready.pdf"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/webcnn_teaser.jpg'/></a></div></td>
		<td class="pub_td2">Ali Jahanian, Phillip Isola, Donglai Wei<br><b>Mining Visual Evolution in 21 Years of Web Design</b><br><i>CHI Extended Abstract, 2017</i>.<br>[<a href="http://people.csail.mit.edu/jahanian/papers/JahanianIsolaWei-designEvolution-CHI2017-LBW-camera-ready.pdf">Paper</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="https://phillipi.github.io/pix2pix/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/pix2pix_teaser.jpg'/></a></div></td>
		<td class="pub_td2">Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros<br><b>Image-to-Image Translation with Conditional Adversarial Networks</b><br><i>CVPR</i>, 2017.<br>[<a href="https://arxiv.org/abs/1611.07004">Paper</a>][<a href="https://phillipi.github.io/pix2pix/">Website</a>][<a href="https://github.com/phillipi/pix2pix">Code</a>][<a href="https://affinelayer.com/pixsrv/">Demo</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="https://richzhang.github.io/splitbrainauto/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/splitbrain_teaser.jpg'/></a></div></td>
		<td class="pub_td2">Richard Zhang, Phillip Isola, Alexei A. Efros<br><b>Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction</b><br><i>CVPR</i>, 2017.<br>[<a href="https://arxiv.org/pdf/1611.09842v1.pdf">Paper</a>][<a href="https://richzhang.github.io/splitbrainauto/">Website</a>][<a href="https://github.com/richzhang/splitbrainauto/">Code</a>]
	</td></tr>

  <tr><td class="year_heading"><br>2016<hr></td></tr>
	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://richzhang.github.io/colorization/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/cic_teaser.jpg'/></a></div></td>
		<td class="pub_td2">Richard Zhang, Phillip Isola, Alexei A. Efros<br><b>Colorful Image Colorization</b><br><i>European Conference on Computer Vision (ECCV)</i>, 2016 (oral).<br>[<a href="http://richzhang.github.io/colorization/resources/colorful_eccv2016.pdf">Paper</a>][<a href="http://richzhang.github.io/colorization/">Website</a>][<a href="https://github.com/richzhang/colorization">Code</a>][<a href="http://demos.algorithmia.com/colorize-photos/">Demo</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://andrewowens.org/vis/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/VIS_teaser.jpg'/></a></div></td>
		<td class="pub_td2">Andrew Owens, Phillip Isola, Josh McDermott, Antonio Torralba, Edward H. Adelson, William T. Freeman<br><b>Visually Indicated Sounds</b><br><i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2016 (oral).<br>[<a href="http://arxiv.org/pdf/1512.08512">Paper</a>][<a href="http://andrewowens.org/vis/">Website</a>][<a href="https://www.youtube.com/watch?v=0FW99AQmMc8">Video</a>][<a href="http://vis.csail.mit.edu/vis-data.zip">Dataset (50GB)</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://arxiv.org/abs/1511.06811"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/cooccurrences_teaser.jpg'/></a></div></td>
		<td class="pub_td2">Phillip Isola, Daniel Zoran, Dilip Krishnan, Edward H. Adelson<br><b>Learning Visual Groups From Co-occurrences in Space and Time</b><br><i>International Conference on Learning Representations, Workshop paper</i>, 2016<br>[<a href="http://arxiv.org/abs/1511.06811">Paper</a>]
	</td></tr>

  <tr><td class="year_heading"><br>2015<hr></td></tr>
	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="https://dilipkay.wordpress.com/ordinals/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/ordinal_teaser.jpg'/></a></td>
		<td class="pub_td2">Daniel Zoran, Phillip Isola, Dilip Krishnan, William T. Freeman<br><b>Learning Ordinal Relationships for Mid-Level Vision</b><br><i>International Conference on Computer Vision (ICCV)</i>, 2015.<br>[<a href="http://people.csail.mit.edu/danielzoran/ordinal.pdf">Paper</a>][<a href="https://dilipkay.wordpress.com/ordinals/">Website</a>][<a href="https://www.dropbox.com/s/nbtf8f31rm9858b/learning_ordinal_relationships_code.zip?dl=0">Code</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://web.mit.edu/phillipi/Public/states_and_transformations/index.html"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/ST_teaser.jpg'/></a></div></td>
		<td class="pub_td2">Phillip Isola*, Joseph J. Lim*, Edward H. Adelson<br><b>Discovering States and Transformations in Image Collections</b><br><i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2015 (oral).<br>[<a href="http://people.csail.mit.edu/lim/paper/state_cvpr15.pdf">Paper</a>][<a href="http://web.mit.edu/phillipi/Public/states_and_transformations/index.html">Website</a>][<a href="http://wednesday.csail.mit.edu/joseph_result/state_and_transformation/release_dataset.zip">Dataset</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://figrim.mit.edu/"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/figrim_teaser.jpg'/></a></div></td>
		<td class="pub_td2">Zoya Bylinskii, Phillip Isola, Constance Bainbridge, Antonio Torralba, Aude Oliva<br><b>Intrinsic and Extrinsic Effects on Image Memorability</b><br><i>Vision Research</i>, 2015.<br>[<a href="http://figrim.mit.edu/intrinsic_extrinsic_memorability.pdf">Paper</a>][<a href="http://figrim.mit.edu/">Website</a>][<a href="https://github.com/cvzoya/memorability-distinctiveness">Code</a>][<a href="http://figrim.mit.edu/index_eyetracking.html">Dataset</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://arxiv.org/abs/1412.7884"><img class="teaser_img" style="width:200px" src='http://web.mit.edu/phillipi/www/images/sparklevision.jpg'/></a></td>
		<td class="pub_td2">Zhengdong Zhang, Phillip Isola, Edward H. Adelson<br><b>Sparkle Vision: Seeing the World through Random Specular Microfacets</b><br><i>4th IEEE International Workshop on Computational Cameras and Displays</i>, 2015.<br>[<a href="http://arxiv.org/abs/1412.7884">Paper</a>]
	</td></tr>

  <tr><td class="year_heading"><br>2014<hr></td></tr>
	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://web.mit.edu/phillipi/Public/crisp_boundaries/index.html"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/crisp_boundaries_teaser.jpg'/></a></div></td>
		<td class="pub_td2">Phillip Isola, Daniel Zoran, Dilip Krishnan, Edward H. Adelson<br><b>Crisp Boundary Detection Using Pointwise Mutual Information</b><br><i>European Conference on Computer Vision (ECCV)</i>, 2014 (oral).<br>[<a href="http://web.mit.edu/phillipi/www/publications/crisp_boundaries.pdf">Paper</a>][<a href="http://web.mit.edu/phillipi/Public/crisp_boundaries/index.html">Website</a>][<a href="https://github.com/phillipi/crisp-boundaries">Code</a>]
	</td></tr>

	<tr><td class="year_heading"><br>2013<hr></td></tr>
	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://web.mit.edu/phillipi/www/publications/memory_pami.pdf"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/memorability_teaser.jpg'/></a></div></td>
		<td class="pub_td2">Phillip Isola, Jianxiong Xiao, Devi Parikh, Antonio Torralba, Aude Oliva<br><b>What Makes a Photograph Memorable?</b><br><i> Pattern Analysis and Machine Intelligence (PAMI)</i>, 2013.<br>[<a href="http://web.mit.edu/phillipi/www/publications/memory_pami.pdf">Paper</a>][<a href="http://web.mit.edu/phillipi/Public/MemorabilityPAMI/index.html">Website</a>][<a href="http://people.csail.mit.edu/phillipi/Image%20memorability/cvpr_memorability_data.zip">Dataset</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://web.mit.edu/phillipi/www/SceneCollaging/index.html"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/scene_collaging_teaser.jpg'/></a></div></td>
		<td class="pub_td2">Phillip Isola and Ce Liu<br><b>Scene Collaging: Analysis and Synthesis of Natural Images with Semantic Layers.</b><br><i> International Conference on Computer Vision (ICCV)</i>, 2013.<br>[<a href="http://web.mit.edu/phillipi/www/publications/scene_collaging.pdf">Paper</a>][<a href="http://web.mit.edu/phillipi/Public/SceneCollaging/index.html">Website</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://vcg.seas.harvard.edu/publications/what-makes-visualization-memorable"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/infoviz_memorability_teaser.jpg'/></a></div></td>
		<td class="pub_td2">Michelle Borkin, Azalea Vo, Zoya Bylinskii, Phillip Isola, Shashank Sunkavalli, Aude Oliva, Hanspeter Pfister<br><b>What Makes a Visualization Memorable?</b><br><i> IEEE Transactions on Visualization and Computer Graphics (Infovis)</i>, 2013.<br>[<a href="http://vcg.seas.harvard.edu/files/pfister/files/infovis_borkin-128-camera_ready_0.pdf">Paper</a>][<a href="http://vcg.seas.harvard.edu/publications/what-makes-visualization-memorable">Website</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href = "http://www.wilmabainbridge.com/facememorability.html"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/face_memorability_teaser.jpg'/></a></div></td>
		<td class="pub_td2">Wilma A. Bainbridge, Phillip Isola, Aude Oliva<br><b>The Intrinsic Memorability of Face Images.</b><br><i> Journal of Experimental Psychology: General</i>, 2013.<br>[<a href="http://www.wilmabainbridge.com/papers/jepg-2013.pdf">Paper</a>][<a href = "http://www.wilmabainbridge.com/facememorability.html">Website</a>][<a href="http://www.wilmabainbridge.com/facememorability2.html">Dataset</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://spie.org/x93734.xml"><img class="teaser_img" style="width:128px" src='http://web.mit.edu/phillipi/www/images/spie_memorability_teaser.jpg'/><a/></div></td>
		<td class="pub_td2">Aude Oliva, Phillip Isola, Aditya Khosla, Wilma A. Bainbridge<br><b>What makes a picture memorable?</b><br><i>SPIE Newsroom Article</i>, 2013.<br>[<a href="http://spie.org/x93734.xml">Article</a>]
	</td></tr>

  <tr><td class="year_heading"><br>2012<hr></td></tr>
	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://people.csail.mit.edu/khosla/papers/sa2012_khosla.pdf"><img class="teaser_img" style="width:224px" src='http://web.mit.edu/phillipi/www/images/siggraph_khosla.jpg'/></a></div></td>
		<td class="pub_td2">Aditya Khosla*, Jianxiong Xiao*, Phillip Isola, Antonio Torralba, Aude Oliva<br><b>Image Memorability and Visual Inception.</b><br><i>SIGGRAPH Asia Technical Briefs (Invited Paper)</i>, 2012.<br>[<a href="http://people.csail.mit.edu/khosla/papers/sa2012_khosla.pdf">Paper</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://people.csail.mit.edu/fcole/shapecollage/index.html"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/shape_collage_teaser.jpg'/></a></div></td>
		<td class="pub_td2">Forrester Cole, Phillip Isola, William T. Freeman, Fredo Durand, Edward H. Adelson<br><b>Shapecollage: Occlusion-aware, example-based shape interpretation.</b><br><i> European Conference on Computer Vision (ECCV)</i>, 2012.<br>[<a href="http://people.csail.mit.edu/fcole/shapecollage/shapecollage.pdf">Paper</a>][<a href="http://people.csail.mit.edu/fcole/shapecollage/index.html">Website</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://www.wilmabainbridge.com/facememorability.html"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/face_memorability_teaser.jpg'/></a></div></td>
		<td class="pub_td2">Wilma A. Bainbridge*, Phillip Isola*, Idan Blank, Aude Oliva<br><b>Establishing a database for studying human face photograph memorability.</b><br><i> Proceedings of the Annual Conference of the Cognitive Sciences Society</i>, 2012.<br>[<a href="http://www.wilmabainbridge.com/papers/cogsci-2012.pdf">Paper</a>][<a href="http://www.wilmabainbridge.com/facememorability.html">Website</a>][<a href="http://www.wilmabainbridge.com/facememorability2.html">Dataset</a>]
	</td></tr>

  <tr><td class="year_heading"><br>2011<hr></td></tr>
	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://web.mit.edu/phillipi/www/UnderstandingMemorability/index.html"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/NIPS_memorability_teaser.jpg'/></a></div></td>
		<td class="pub_td2">Phillip Isola, Devi Parikh, Antonio Torralba, Aude Oliva<br><b>Understanding the intrinsic memorability of images.</b><br><i> NeurIPS</i>, 2011.<br>[<a href="http://web.mit.edu/phillipi/www/publications/UnderstandingMemorability.pdf">Paper</a>]<a href="http://web.mit.edu/phillipi/Public/UnderstandingMemorability/index.html">Website</a>]
	</td></tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://web.mit.edu/phillipi/www/WhatMakesAnImageMemorable/index.html"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/memorability_teaser.jpg'/></a></div></td>
		<td class="pub_td2">Phillip Isola, Jianxiong Xiao, Antonio Torralba, Aude Oliva<br><b>What makes an image memorable?</b><br><i> IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2011.<br>[<a href="http://web.mit.edu/phillipi/www/publications/WhatMakesAnImageMemorable.pdf">Paper</a>][<a href="http://web.mit.edu/phillipi/Public/WhatMakesAnImageMemorable/index.html">Website</a>][<a href="http://people.csail.mit.edu/phillipi/Image%20memorability/cvpr_memorability_data.zip">Dataset</a>]
	</td></tr>

  <tr><td class="year_heading"><br>2008<hr></td></tr>
	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://web.mit.edu/phillipi/www/publications/Turk-Browne_JEPLMC_2008.pdf"><img class="teaser_img" style="width:180px" src='http://web.mit.edu/phillipi/www/images/MVSL_teaser.jpg'/></a></div></td>
		<td class="pub_td2">Nicholas B. Turk-Browne, Phillip Isola, Brian J. Scholl, Teresa A. Treat<br><b>Multidimensional visual statistical learning.</b><br><i> Journal of Experimental Psychology: Learning, Memory, and Cognition</i>, 2008.<br>[<a href="http://web.mit.edu/phillipi/www/publications/Turk-Browne_JEPLMC_2008.pdf">Paper</a>]
	</td></tr>

	</table>

	</div>


	<h2>Other recent projects</h2><p>
	<a href="http://web.mit.edu/phillipi/www/Public/MAS.531/Final%20project/final%20website/final_website.html">Seeing sight</a> -- Final project for MAS.531. Annotates what is seen by one camera from the perspective of another camera.<br><br>

	<a href="http://web.mit.edu/phillipi/www/Public/sfs/Bayesian%20Shape%20from%20Shading.pdf">Bayesian generative shape from shading</a> -- Final project for 9.660. Infers simple bumpy shapes with a generative model.<br><br>

	<a href="http://web.mit.edu/phillipi/www/apps.html">Apps</a> -- A few simple apps written in Processing.<br><br>

	<a href="http://www.flickr.com/photos/27639271@N07/sets/">Photos</a> -- flickr site with photos.<br><br>

	<a href="http://www.wolfire.com/overgrowth">Overgrowth</a> -- Upcoming game from <a href="http://www.wolfire.com">Wolfire Games</a>, where I worked for a year, mainly on the <a href="http://www.youtube.com/watch?v=taX4h3UajBc">map editor</a> and <a href="http://blog.wolfire.com/2009/07/sky-and-lighting-editing-part-1/">other tools</a> for this game. They have an interesting, and very revealing, development blog <a href="http://blog.wolfire.com">here</a>!
		-->

<!--
<hr>
<center><a href="http://accessibility.mit.edu/">Accessibility</a></center>
-->

</body>

</html>
